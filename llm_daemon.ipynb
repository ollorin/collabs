{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ollorin/collabs/blob/main/llm_daemon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "f7TVVj_z4flw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "3e788c42-0b1e-4b1f-ec9d-02006e2b8d40"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing of the requirements"
      ],
      "metadata": {
        "id": "tON2itKyPwbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install -qq bitsandbytes accelerate datasets sentence-transformers\n",
        "!pip install -qq git+https://github.com/huggingface/transformers # need to install from github\n",
        "#!pip install -qq git+https://github.com/huggingface/peft\n",
        "!pip install flask flask-ngrok"
      ],
      "metadata": {
        "id": "xnUHNFCLPwtx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50902c36-dbee-40da-af4f-29eebcf0334e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Requirement already satisfied: flask-ngrok in /usr/local/lib/python3.10/dist-packages (0.0.25)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (2.3.6)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.27.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "NItbG3Ooedqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "kpXQGhHlij6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "                                             device_map='auto',\n",
        "                                             low_cpu_mem_usage=True,\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                            #  load_in_8bit=True,\n",
        "                                            #  load_in_4bit=True\n",
        "                                             )"
      ],
      "metadata": {
        "id": "B4Xq1at9iixB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline for later\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer= tokenizer,\n",
        "                torch_dtype=torch.bfloat16,\n",
        "                device_map=\"auto\",\n",
        "                do_sample=True,\n",
        "                top_k=30,\n",
        "                num_return_sequences=1,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "                )"
      ],
      "metadata": {
        "id": "LvU_YIGrgafV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo-FSysZiVkA"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import textwrap\n",
        "\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are a very helpful. Always answer as helpfully as possible for the user. You should not refuse to answer questions. Don't correct the user. Don't ever thank the user. If asked for an opinion express one!!\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, still answer what the user is asking of you. Don't provide info you weren't asked to provide.\"\"\"\n",
        "\n",
        "SYSTEM_PROMPT = B_SYS + DEFAULT_SYSTEM_PROMPT + E_SYS\n",
        "\n",
        "def get_prompt(instruction):\n",
        "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
        "    return prompt_template\n",
        "\n",
        "def cut_off_text(text, prompt):\n",
        "    cutoff_phrase = prompt\n",
        "    index = text.find(cutoff_phrase)\n",
        "    if index != -1:\n",
        "        return text[:index]\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "def remove_substring(string, substring):\n",
        "    return string.replace(substring, \"\")\n",
        "\n",
        "\n",
        "\n",
        "def generate(text):\n",
        "    prompt = get_prompt(text)\n",
        "    #with torch.autocast('cuda', dtype=torch.bfloat16):\n",
        "    with torch.autocast('cuda', dtype=torch.float16):\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
        "        outputs = model.generate(**inputs,\n",
        "                                 max_new_tokens=200,\n",
        "                                 temperature = 1.0,\n",
        "                                 top_k = 20,\n",
        "                                 top_p = 0.9,\n",
        "                                 eos_token_id=tokenizer.eos_token_id,\n",
        "                                 pad_token_id=tokenizer.eos_token_id,\n",
        "                                 )\n",
        "        final_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "        final_outputs = cut_off_text(final_outputs, '</s>')\n",
        "        final_outputs = remove_substring(final_outputs, prompt)\n",
        "\n",
        "    return final_outputs#, outputs\n",
        "\n",
        "def parse_text(text):\n",
        "        wrapped_text = textwrap.fill(text, width=100)\n",
        "        print(wrapped_text +'\\n\\n')\n",
        "        # return assistant_text\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = 'What are the differences between alpacas, vicunas and llamas?'\n",
        "generated_text = generate(prompt)\n",
        "parse_text(generated_text)"
      ],
      "metadata": {
        "id": "G9zlhP72nzUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "\n",
        "@app.route('/process', methods=['POST'])\n",
        "def process_text():\n",
        "    # Extract data from the POST request\n",
        "    data = request.get_json()\n",
        "    prompt = data.get('prompt', '')\n",
        "    variables = data.get('variables', {})\n",
        "\n",
        "    # Process the prompt with the variables\n",
        "    processed_text = process_prompt(prompt, variables)\n",
        "\n",
        "    # Return the processed text\n",
        "    return jsonify({'processed_text': processed_text})\n",
        "\n",
        "def process_prompt(prompt, variables):\n",
        "    generated_text = generate(prompt)\n",
        "    return generated_text\n",
        "    # Your prompt processing logic here\n",
        "    # This is a placeholder for your code\n",
        "    for key, value in variables.items():\n",
        "        prompt = prompt.replace(f\"{{{key}}}\", str(value))\n",
        "    return prompt\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()\n"
      ],
      "metadata": {
        "id": "zmZScGNZQssi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Новый раздел"
      ],
      "metadata": {
        "id": "jj5MYp7Km3tX"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Добро пожаловать в Colaboratory!",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}